{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/nmduy/CA683/Credit Project')\n",
    "#from fancyimpute import IterativeImputer as MICE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score \n",
    "import random\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ DATA AND REMOVE NA\n",
    "Read CSV and only keep the full-info cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(307511, 110)\n(307511, 110)\n"
    }
   ],
   "source": [
    "data = pd.read_csv('data/imputed_train.csv')\n",
    "data_remove_id = data.drop(['SK_ID_CURR'], axis=1) if 'SK_ID_CURR' in list(data.columns) else data.copy()\n",
    "data_remove_id = data_remove_id.drop(['DAYS_EMPLOYED_ANOM'], axis=1) if 'DAYS_EMPLOYED_ANOM' in list(data_remove_id.columns) else data_remove_id.copy()\n",
    "print(data_remove_id.shape)\n",
    "data_remove_id = data_remove_id.dropna()\n",
    "data_remove_id = data_remove_id.reset_index(drop=True)\n",
    "print(data_remove_id.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKE IT BALANCE\n",
    "Create subset and maybe make it balance between classes within classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Numb train/val/test samples: 238320 / 64225 / 4966\n(238320, 110)\n"
    }
   ],
   "source": [
    "##### MAKE IT BALANCE #####\n",
    "random.seed(1509)\n",
    "\n",
    "numb_pos_samples = data_remove_id['TARGET'].sum()\n",
    "numb_neg_samples = len(data_remove_id) - numb_pos_samples\n",
    "portion_pos_train = 0.8\n",
    "portion_pos_val = 0.1\n",
    "numb_pos_train = int(numb_pos_samples * portion_pos_train)\n",
    "numb_pos_val = int(numb_pos_samples * portion_pos_val)\n",
    "numb_pos_test = numb_pos_samples - numb_pos_train - numb_pos_val\n",
    "\n",
    "index_pos = data_remove_id.index[data_remove_id['TARGET'] == 1].tolist()\n",
    "index_neg = [i for i in range(len(data_remove_id)) if i not in index_pos]\n",
    "\n",
    "data_pos = data_remove_id.iloc[np.asarray(index_pos), :]\n",
    "data_neg = data_remove_id.iloc[np.asarray(index_neg), :]\n",
    "\n",
    "##### shuffle #####\n",
    "data_pos = data_pos.sample(frac=1)\n",
    "data_neg = data_neg.sample(frac=1)\n",
    "\n",
    "data_pos_train = data_pos.iloc[0:numb_pos_train,:]\n",
    "data_pos_val = data_pos.iloc[numb_pos_train:numb_pos_train + numb_pos_val, :]\n",
    "data_pos_test = data_pos.iloc[numb_pos_train + numb_pos_val:, :]\n",
    "\n",
    "data_neg_train = data_neg.iloc[0:int(numb_pos_train*11), :]\n",
    "data_neg_val = data_neg.iloc[int(numb_pos_train*11):-numb_pos_test, :]\n",
    "data_neg_test = data_neg.iloc[-numb_pos_test:, :]\n",
    "\n",
    "data_train = pd.concat([data_pos_train, data_neg_train], ignore_index=True)\n",
    "data_val = pd.concat([data_pos_val, data_neg_val], ignore_index=True)\n",
    "data_not_test = pd.concat([data_train, data_val], ignore_index=True)\n",
    "data_test = pd.concat([data_pos_test, data_neg_test], ignore_index=True)\n",
    "\n",
    "print(f\"Numb train/val/test samples: {len(data_train)} / {len(data_val)} / {len(data_test)}\")\n",
    "\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE FT CONTAIN ONLY 1 VALUES\n",
    "Sometimes the binary features contains only 1 value due to the dropna functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[]\n0\n"
    }
   ],
   "source": [
    "##### REMOVE 1-VALUE FT #####\n",
    "remove_ft = []\n",
    "for col in list(data_not_test.drop('TARGET', axis=1).columns):\n",
    "    if data_not_test[col].sum() == 0 or data_not_test[col].sum() == len(data_not_test[col]):\n",
    "        remove_ft += [col]\n",
    "\n",
    "print(remove_ft)        \n",
    "print(len(remove_ft))\n",
    "\n",
    "data_train = data_train.drop(remove_ft, axis=1)\n",
    "data_val = data_val.drop(remove_ft, axis=1)\n",
    "data_test = data_test.drop(remove_ft, axis=1)\n",
    "data_not_test = data_not_test.drop(remove_ft, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE COLLINEAR\n",
    "Some of ft still have a strong relationship with others --> Need to remove one of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FIND CORRELATION MATRIX #####\n",
    "corr_matrix = data_not_test.corr()\n",
    "corr_matrix = corr_matrix.replace(1, 0)\n",
    "\n",
    "corr_max = corr_matrix.max(axis = 0)\n",
    "corr_min = corr_matrix.min(axis = 0)\n",
    "corr_max_idx = corr_matrix.idxmax(axis = 0)\n",
    "corr_min_idx = corr_matrix.idxmin(axis = 0)\n",
    "\n",
    "dict_corr_max = dict(corr_max)\n",
    "dict_corr_max_idx = dict(corr_max_idx)\n",
    "dict_corr_max_filter = {}\n",
    "for x in dict_corr_max.keys():\n",
    "    if dict_corr_max[x] > 0.8:\n",
    "        dict_corr_max_filter[x] = dict_corr_max[x]\n",
    "dict_corr_max_idx_filter = {}\n",
    "for x in dict_corr_max_filter.keys():\n",
    "   dict_corr_max_idx_filter[x] = dict_corr_max_idx[x]\n",
    "\n",
    "dict_corr_min = dict(corr_min)\n",
    "dict_corr_min_idx = dict(corr_min_idx)\n",
    "dict_corr_min_filter = {}\n",
    "for x in dict_corr_min.keys():\n",
    "    if dict_corr_min[x] < -0.8:\n",
    "        dict_corr_min_filter[x] = dict_corr_min[x]\n",
    "dict_corr_min_idx_filter = {}\n",
    "for x in dict_corr_min_filter.keys():\n",
    "   dict_corr_min_idx_filter[x] = dict_corr_min_idx[x]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict_corr_max_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict_corr_min_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(26872, 88)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "##### REMOVE COLLINEAR ####\n",
    "# remove_ft = ['AMT_GOODS_PRICE', 'REGION_RATING_CLIENT_W_CITY', 'OBS_60_CNT_SOCIAL_CIRCLE', 'CNT_CHILDREN', \n",
    "#              'LIVE_REGION_NOT_WORK_REGION', 'NAME_INCOME_TYPE_Working', 'NAME_EDUCATION_TYPE_Secondary / secondary special']\n",
    "# remove_ft = ['LIVE_REGION_NOT_WORK_REGION', 'DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "# remove_ft = ['AMT_GOODS_PRICE', 'REGION_RATING_CLIENT_W_CITY', 'OBS_60_CNT_SOCIAL_CIRCLE']\n",
    "remove_ft = []\n",
    "if len(remove_ft) > 0:\n",
    "    data_train = data_train.drop(remove_ft, axis=1)\n",
    "    data_test = data_test.drop(remove_ft, axis=1)\n",
    "    data_val = data_val.drop(remove_ft, axis=1)\n",
    "    data_not_test = data_not_test.drop(remove_ft, axis=1)\n",
    "\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE DOMINANT FEATURES\n",
    "Dominant features are features containing almost only 1 values in entire dataset --> SHOULD REMOVE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_name = ['NAME_CONTRACT_TYPE', 'CODE_GENDER']\n",
    "pc_pos = {}\n",
    "for col in list(data_not_test.columns):\n",
    "    if 'NAME' in col or 'OCCUPATION' in col or 'WEEKDAY' in col or 'ORGANIZATION' in col:\n",
    "        categorical_name += [col]\n",
    "    if col in categorical_name:\n",
    "        values = np.asarray(list(data_remove_id[col]))\n",
    "        pc_pos[col] = np.sum(values) / len(values)\n",
    "remove_ft_dominant = []\n",
    "for col in categorical_name:\n",
    "    if pc_pos[col] < 0.01 or pc_pos[col] > 0.99:\n",
    "        remove_ft_dominant += [col]\n",
    "        \n",
    "print(len(remove_ft_dominant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REMOVE DOMINANT FT IN BINARY #####\n",
    "data_train = data_\btrain.drop(remove_ft_dominant, axis=1)\n",
    "data_val = data_val.drop(remove_ft_dominant, axis=1)\n",
    "data_test = data_test.drop(remove_ft_dominant, axis=1)\n",
    "data_not_test = data_not_test.drop(remove_ft_dominant, axis=1)\n",
    "\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(238320, 109)\n"
    }
   ],
   "source": [
    "X_train = data_train.drop(['TARGET'], axis = 1)\n",
    "X_test = data_test.drop(['TARGET'], axis = 1)\n",
    "X_val = data_val.drop(['TARGET'], axis = 1)\n",
    "y_train = data_train['TARGET']\n",
    "y_test = data_test['TARGET']\n",
    "y_val = data_val['TARGET']\n",
    "X_not_test = data_not_test.drop(['TARGET'], axis = 1)\n",
    "y_not_test = data_not_test['TARGET']\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogisticRegression(C=1000000000.0, class_weight={0: 1, 1: 12}, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=500, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)"
     },
     "metadata": {},
     "execution_count": 213
    }
   ],
   "source": [
    "logmodel = LogisticRegression(C=1e9, max_iter=500, class_weight={0:1, 1:12})\n",
    "logmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric(gtnp, pdnp):\n",
    "    # input are numpy vector\n",
    "    total_samples = len(gtnp)\n",
    "    #print(f\"Total sample: {total_samples}\")\n",
    "    total_correct = np.sum(gtnp == pdnp)\n",
    "    accuracy = total_correct / total_samples\n",
    "    gt_pos = np.where(gtnp == 1)[0]\n",
    "    gt_neg = np.where(gtnp == 0)[0]\n",
    "    TP = np.sum(pdnp[gt_pos])\n",
    "    TN = np.sum(1 - pdnp[gt_neg])\n",
    "    FP = np.sum(pdnp[gt_neg])\n",
    "    FN = np.sum(1 - pdnp[gt_pos])\n",
    "    precision = TP / (TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = str(accuracy)\n",
    "    metrics['precision'] = str(precision)\n",
    "    metrics['recall'] = str(recall)\n",
    "    metrics['f1'] = str(f1)\n",
    "    metrics['tp'] = str(int(TP))\n",
    "    metrics['tn'] = str(int(TN))\n",
    "    metrics['fp'] = str(int(FP))\n",
    "    metrics['fn'] = str(int(FN))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'accuracy': '0.5412932192010742',\n 'precision': '0.1037394023689083',\n 'recall': '0.5896273917421954',\n 'f1': '0.17643646554516756',\n 'tp': '11710',\n 'tn': '117291',\n 'fp': '101169',\n 'fn': '8150',\n 'auc': 0.5632632976288565}"
     },
     "metadata": {},
     "execution_count": 215
    }
   ],
   "source": [
    "##### TRAIN PREDICTION #####\n",
    "train_predict = logmodel.predict(X_train)\n",
    "metric = calculate_metric(np.asarray(list(y_train)), train_predict)\n",
    "metric['auc'] = roc_auc_score(y_train,train_predict)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'accuracy': '0.5383261969637991',\n 'precision': '0.04813225559658051',\n 'recall': '0.5829975825946817',\n 'f1': '0.08892302965125211',\n 'tp': '1447',\n 'tn': '33127',\n 'fp': '28616',\n 'fn': '1035',\n 'auc': 0.5597640197442902}"
     },
     "metadata": {},
     "execution_count": 216
    }
   ],
   "source": [
    "##### VAL PREDICTION #####\n",
    "val_predict = logmodel.predict(X_val)\n",
    "metric = calculate_metric(np.asarray(list(y_val)), val_predict)\n",
    "metric['auc'] = roc_auc_score(y_val,val_predict)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'accuracy': '0.5712847362062021',\n 'precision': '0.56855151045701',\n 'recall': '0.5912202980265807',\n 'f1': '0.5796643632773938',\n 'tp': '1468',\n 'tn': '1369',\n 'fp': '1114',\n 'fn': '1015',\n 'auc': 0.5712847362062021}"
     },
     "metadata": {},
     "execution_count": 217
    }
   ],
   "source": [
    "##### TEST PREDICTION #####\n",
    "test_predict = logmodel.predict(X_test)\n",
    "metric = calculate_metric(np.asarray(list(y_test)), test_predict)\n",
    "metric['auc'] = roc_auc_score(y_test, test_predict)\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPROVE BY ADDING BIAS TERM\n",
    "Assume that we know the portion of positive in training and population, we can adjust the bias term and achieve higher performance (if possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_intercept = logmodel.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADJUST BIAS TERM #####\n",
    "pop_portion = 0.5\n",
    "sample_portion = 0.02 # if training data is balance\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel.intercept_ = origin_intercept - np.log(((1-pop_portion)/pop_portion) * (sample_portion/(1-sample_portion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([3.89182052])"
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "source": [
    "logmodel.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'accuracy': '0.009826988809289694',\n 'precision': '0.009826988809289694',\n 'recall': '1.0',\n 'f1': '0.019462717709450256',\n 'tp': '1679',\n 'tn': '0',\n 'fp': '169177',\n 'fn': '0'}"
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "source": [
    "##### VAL PREDICTION #####\n",
    "val_predict = logmodel.predict(X_val)\n",
    "metric = calculate_metric(np.asarray(list(y_val)), val_predict)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}