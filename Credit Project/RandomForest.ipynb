{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/nmduy/CA683/Credit Project')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "random.seed(1509)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ DATA AND REMOVE NA\n",
    "Read CSV and only keep the full-info cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(307511, 110)\n(307511, 110)\n"
    }
   ],
   "source": [
    "data = pd.read_csv('data/imputed_train.csv')\n",
    "data_remove_id = data.drop(['SK_ID_CURR'], axis=1) if 'SK_ID_CURR' in list(data.columns) else data.copy()\n",
    "data_remove_id = data_remove_id.drop(['DAYS_EMPLOYED_ANOM'], axis=1) if 'DAYS_EMPLOYED_ANOM' in list(data_remove_id.columns) else data_remove_id.copy()\n",
    "print(data_remove_id.shape)\n",
    "data_remove_id = data_remove_id.dropna()\n",
    "data_remove_id = data_remove_id.reset_index(drop=True)\n",
    "print(data_remove_id.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKE IT BALANCE\n",
    "Create subset and maybe make it balance between classes within classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Numb train/val/test samples: 238320 / 64225 / 4966\n(238320, 110)\n"
    }
   ],
   "source": [
    "##### MAKE IT BALANCE #####\n",
    "random.seed(1509)\n",
    "\n",
    "numb_pos_samples = data_remove_id['TARGET'].sum()\n",
    "numb_neg_samples = len(data_remove_id) - numb_pos_samples\n",
    "portion_pos_train = 0.8\n",
    "portion_pos_val = 0.1\n",
    "numb_pos_train = int(numb_pos_samples * portion_pos_train)\n",
    "numb_pos_val = int(numb_pos_samples * portion_pos_val)\n",
    "numb_pos_test = numb_pos_samples - numb_pos_train - numb_pos_val\n",
    "\n",
    "index_pos = data_remove_id.index[data_remove_id['TARGET'] == 1].tolist()\n",
    "index_neg = [i for i in range(len(data_remove_id)) if i not in index_pos]\n",
    "\n",
    "data_pos = data_remove_id.iloc[np.asarray(index_pos), :]\n",
    "data_neg = data_remove_id.iloc[np.asarray(index_neg), :]\n",
    "\n",
    "##### shuffle #####\n",
    "data_pos = data_pos.sample(frac=1)\n",
    "data_neg = data_neg.sample(frac=1)\n",
    "\n",
    "data_pos_train = data_pos.iloc[0:numb_pos_train,:]\n",
    "data_pos_val = data_pos.iloc[numb_pos_train:numb_pos_train + numb_pos_val, :]\n",
    "data_pos_test = data_pos.iloc[numb_pos_train + numb_pos_val:, :]\n",
    "\n",
    "data_neg_train = data_neg.iloc[0:int(numb_pos_train*11), :]\n",
    "data_neg_val = data_neg.iloc[int(numb_pos_train*11):-numb_pos_test, :]\n",
    "data_neg_test = data_neg.iloc[-numb_pos_test:, :]\n",
    "\n",
    "data_train = pd.concat([data_pos_train, data_neg_train], ignore_index=True)\n",
    "data_val = pd.concat([data_pos_val, data_neg_val], ignore_index=True)\n",
    "data_not_test = pd.concat([data_train, data_val], ignore_index=True)\n",
    "data_test = pd.concat([data_pos_test, data_neg_test], ignore_index=True)\n",
    "\n",
    "print(f\"Numb train/val/test samples: {len(data_train)} / {len(data_val)} / {len(data_test)}\")\n",
    "\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE FT CONTAIN ONLY 1 VALUES\n",
    "Sometimes the binary features contains only 1 value due to the dropna functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "##### REMOVE 1-VALUE FT #####\n",
    "remove_ft = []\n",
    "for col in list(data_not_test.drop('TARGET', axis=1).columns):\n",
    "    if data_not_test[col].sum() == 0 or data_not_test[col].sum() == len(data_not_test[col]):\n",
    "        remove_ft += [col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[]\n0\n"
    }
   ],
   "source": [
    "print(remove_ft)        \n",
    "print(len(remove_ft))\n",
    "\n",
    "data_train = data_train.drop(remove_ft, axis=1)\n",
    "data_val = data_val.drop(remove_ft, axis=1)\n",
    "data_test = data_test.drop(remove_ft, axis=1)\n",
    "data_not_test = data_not_test.drop(remove_ft, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['FLAG_MOBIL', 'FLAG_DOCUMENT_2']"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "remove_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(238320, 109)\n"
    }
   ],
   "source": [
    "X_train = data_train.drop(['TARGET'], axis = 1)\n",
    "X_test = data_test.drop(['TARGET'], axis = 1)\n",
    "X_val = data_val.drop(['TARGET'], axis = 1)\n",
    "y_train = data_train['TARGET']\n",
    "y_test = data_test['TARGET']\n",
    "y_val = data_val['TARGET']\n",
    "X_not_test = data_not_test.drop(['TARGET'], axis = 1)\n",
    "y_not_test = data_not_test['TARGET']\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(class_weight='balanced')\n",
    "rfc.fit(X_train,y_train)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric(gtnp, pdnp):\n",
    "    # input are numpy vector\n",
    "    total_samples = len(gtnp)\n",
    "    #print(f\"Total sample: {total_samples}\")\n",
    "    total_correct = np.sum(gtnp == pdnp)\n",
    "    accuracy = total_correct / total_samples\n",
    "    gt_pos = np.where(gtnp == 1)[0]\n",
    "    gt_neg = np.where(gtnp == 0)[0]\n",
    "    TP = np.sum(pdnp[gt_pos])\n",
    "    TN = np.sum(1 - pdnp[gt_neg])\n",
    "    FP = np.sum(pdnp[gt_neg])\n",
    "    FN = np.sum(1 - pdnp[gt_pos])\n",
    "    precision = TP / (TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = str(accuracy)\n",
    "    metrics['precision'] = str(precision)\n",
    "    metrics['recall'] = str(recall)\n",
    "    metrics['f1'] = str(f1)\n",
    "    metrics['tp'] = str(int(TP))\n",
    "    metrics['tn'] = str(int(TN))\n",
    "    metrics['fp'] = str(int(FP))\n",
    "    metrics['fn'] = str(int(FN))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'accuracy': '0.9999412554548506',\n 'precision': '1.0',\n 'recall': '0.9992950654582075',\n 'f1': '0.9996474084521232',\n 'tp': '19846',\n 'tn': '218460',\n 'fp': '0',\n 'fn': '14',\n 'auc': 0.9996475327291037}"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "##### TRAIN PREDICTION #####\n",
    "train_predict = rfc.predict(X_train)\n",
    "metric = calculate_metric(np.asarray(list(y_train)), train_predict)\n",
    "metric['auc'] = roc_auc_score(y_train,train_predict)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'accuracy': '0.96141689373297',\n 'precision': '0.75',\n 'recall': '0.0024174053182917004',\n 'f1': '0.004819277108433735',\n 'tp': '6',\n 'tn': '61741',\n 'fp': '2',\n 'fn': '2476',\n 'auc': 0.5011925064911592}"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "##### VALIDATE PREDICTION #####\n",
    "val_predict = rfc.predict(X_val)\n",
    "metric = calculate_metric(np.asarray(list(y_val)), val_predict)\n",
    "metric['auc'] = roc_auc_score(y_val,val_predict)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'accuracy': '0.5006041079339508',\n 'precision': '1.0',\n 'recall': '0.0012082158679017317',\n 'f1': '0.0024135156878519713',\n 'tp': '3',\n 'tn': '2483',\n 'fp': '0',\n 'fn': '2480',\n 'auc': 0.5006041079339508}"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "##### TEST PREDICTION #####\n",
    "test_predict = rfc.predict(X_test)\n",
    "metric = calculate_metric(np.asarray(list(y_test)), test_predict)\n",
    "metric['auc'] = roc_auc_score(y_test,test_predict)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_remove_id.drop(['TARGET'], axis = 1)\n",
    "y = data_remove_id['TARGET']\n",
    "rfc_cv_score = cross_val_score(rfc, X, y, cv=10, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=== All F1 Scores ===\n[0.00356083 0.00356083 0.00710059 0.00829384 0.00237389 0.00356718\n 0.00474215 0.00593824 0.00237954 0.00474496]\n\n\n=== Mean F1 Score ===\nMean F1 Score - Random Forest:  0.004626204403960232\n"
    }
   ],
   "source": [
    "print(\"=== All F1 Scores ===\")\n",
    "print(rfc_cv_score)\n",
    "print('\\n')\n",
    "print(\"=== Mean F1 Score ===\")\n",
    "print(\"Mean F1 Score - Random Forest: \", rfc_cv_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}